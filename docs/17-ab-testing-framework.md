# A/B Testing Framework

Complete guide to designing, implementing, and analyzing A/B tests for search relevance experiments.

## ðŸ“‹ Table of Contents
- [Overview](#overview)
- [Experimental Design](#experimental-design)
- [Implementation](#implementation)
- [Statistical Analysis](#statistical-analysis)
- [Azure Integration](#azure-integration)
- [Results Tracking](#results-tracking)
- [Analysis Tools](#analysis-tools)
- [Best Practices](#best-practices)

---

## Overview

### Real-World Success Story: TechDocs Search Transformation

**Company**: TechDocs, technical documentation platform serving 2.5M developers across 18,000 companies

**Scale**: 12M documentation pages, 85M queries/month (33 QPS average, 240 QPS peak), 420K daily active users

**Challenge**: The product team believed that adding semantic search would improve user experience, but stakeholders were skeptical about the engineering investment required ($180K for semantic indexing infrastructure + $45K/month operational cost). The team needed data-driven proof that the improvement would justify the investment.

**Previous Approach: Ship and Pray**

Before implementing their A/B testing framework, TechDocs had made three major search changes in 18 months:

1. **Custom Analyzer Update** (Month 1):
   - Deployed to 100% of users immediately
   - No baseline measurement
   - Support tickets increased 31% in first week
   - **Problem**: Discovered too late that technical term tokenization broke searches like "OAuth2.0"
   - **Impact**: 8 days to identify issue, 3 days to fix, $240K estimated revenue loss

2. **Scoring Profile Enhancement** (Month 8):
   - Deployed based on "testing with sample queries"
   - No statistical validation
   - **Result**: Couldn't measure impact (no control group)
   - Users complained it "felt different" but no data to support reverting or keeping

3. **Vector Search Pilot** (Month 14):
   - Deployed to 10% of users randomly
   - **Problem**: No proper randomization (first 10% of traffic each day = time-of-day bias)
   - **Problem**: Measured for only 3 days (insufficient sample size)
   - **Result**: Inconclusive, abandoned after 2 weeks of confusion

**Total Cost of Failed Experiments**: ~$380K in lost revenue, 6 weeks of engineering time wasted, eroded stakeholder confidence in search team.

---

### The A/B Testing Solution: 8-Week Semantic Search Experiment

**Hypothesis**: Adding semantic search (Azure OpenAI embeddings + hybrid RRF) will improve search success rate (measured by CTR@10) by at least 5% while maintaining zero-result rate below 3%.

**Experiment Design** (Week 1):

The team implemented a rigorous A/B testing framework:

1. **Metrics Selection**:
   - **Primary**: CTR@10 (click-through rate in top 10 results)
     - Baseline: 42.5% (from 90 days of historical data)
     - Target: â‰¥44.6% (+5% relative improvement)
     - MDE (Minimum Detectable Effect): 2 percentage points
   
   - **Secondary Guardrail Metrics**:
     - Zero-result rate (must stay â‰¤3.0%, currently 2.4%)
     - Mean position of first click (lower is better, currently 3.2)
     - Query latency P95 (must stay â‰¤350ms, currently 285ms)
     - Session abandonment rate (must not increase from 18.3%)
   
   - **Business Metrics**:
     - Documentation page views per session
     - Support ticket submission rate (negative signal)
     - User satisfaction score (quarterly survey)

2. **Sample Size Calculation**:
   ```
   Baseline CTR: 42.5%
   Target CTR: 44.6% (5% relative lift)
   Significance level: Î± = 0.05 (95% confidence)
   Statistical power: Î² = 0.80 (80% power)
   
   Required sample size: 24,847 users per variant
   Daily traffic: 420,000 users
   50/50 split: 210,000 users/day per variant
   
   Test duration: 24,847 / 210,000 = 0.12 days
   â†’ Minimum 7 days to capture weekly patterns
   â†’ Planned: 14 days for validation
   ```

3. **Randomization Strategy**:
   - **Unit of randomization**: User ID (not session, ensures consistency)
   - **Hash-based assignment**: MD5(user_id + experiment_id) % 100
     - 0-49: Control (current BM25 search)
     - 50-99: Treatment (hybrid semantic + BM25 with RRF)
   - **Validation**: Pre-experiment A/A test (both variants = BM25)
     - Ran for 3 days with 180K users
     - Verified no statistical difference (p=0.73 for CTR)
     - Confirmed allocation balanced (50.2% / 49.8%)

**Implementation** (Weeks 2-3):

1. **Variant Configurations**:
   
   **Control (Variant A)**:
   ```python
   {
       'query_type': 'full',
       'search_mode': 'all',
       'scoring_profile': 'relevance_v2',
       'select': 'id,title,content,url',
       'top': 10
   }
   ```
   
   **Treatment (Variant B)**:
   ```python
   {
       'query_type': 'semantic',
       'semantic_configuration_name': 'default',
       'query_language': 'en-us',
       'search_mode': 'all',
       'select': 'id,title,content,url,contentVector',
       'top': 10,
       'vector_queries': [{
           'kind': 'vector',
           'vector': <embedding>,
           'fields': 'contentVector',
           'k': 50
       }]
   }
   ```

2. **Instrumentation** (Azure Application Insights):
   - Custom event: `search_query_executed`
     - Properties: user_id, query, variant, timestamp
     - Measurements: result_count, latency_ms
   
   - Custom event: `search_result_clicked`
     - Properties: user_id, query, variant, position, doc_id
     - Measurements: time_to_click_ms
   
   - Automated data pipeline to Cosmos DB
     - Aggregated hourly to `ab_test_metrics` container
     - Real-time dashboard in Power BI

3. **Quality Assurance**:
   - Shadow traffic testing (1 week before launch)
   - Latency validation: P95 = 312ms (within acceptable range <350ms)
   - Cost validation: Vector search added $0.18 per 1K queries (acceptable)
   - Canary deployment: 1% traffic for 24 hours, no errors

**Execution** (Weeks 4-5 - 14 days live experiment):

**Daily Monitoring Checklist**:
- âœ“ Allocation balance (target 50/50, acceptable 48-52%)
- âœ“ Sample size progress (target 24,847 per variant)
- âœ“ Data quality (event logging, no gaps)
- âœ“ Guardrail metrics (zero-result rate, latency)
- âœ“ Error rates (search failures, timeouts)

**Week 1 Results** (Days 1-7):

| Metric | Control | Treatment | Difference | P-value | Status |
|--------|---------|-----------|------------|---------|--------|
| **Sample size** | 1,453,290 | 1,468,710 | - | - | âœ“ Balanced |
| **CTR@10** | 42.3% | 46.8% | +4.5 pp | <0.001 | âœ“ Significant |
| **Zero-result rate** | 2.5% | 1.8% | -0.7 pp | <0.001 | âœ“ Improved |
| **Avg click position** | 3.24 | 2.67 | -0.57 | <0.001 | âœ“ Improved |
| **Latency P95** | 287ms | 316ms | +29ms | <0.001 | âœ“ Acceptable |
| **Abandonment rate** | 18.1% | 15.4% | -2.7 pp | <0.001 | âœ“ Improved |

**Segment Analysis** (discovered critical insight):

Breaking down by query type revealed **semantic search excelled at conceptual queries but degraded exact-match searches**:

| Query Type | % of Queries | Control CTR | Treatment CTR | Difference |
|------------|--------------|-------------|---------------|------------|
| **Exact API names** | 35% | 68.2% | 64.1% | -4.1 pp âš  |
| **Conceptual "how to"** | 42% | 28.4% | 38.7% | +10.3 pp âœ“ |
| **Error messages** | 15% | 52.1% | 58.3% | +6.2 pp âœ“ |
| **Version-specific** | 8% | 45.3% | 41.8% | -3.5 pp âš  |

**Key Finding**: Pure semantic search degraded exact-match queries (API names, version-specific docs).

**Week 2 Adjustment** (Days 8-14):

Based on segment analysis, the team implemented **Variant C: Hybrid RRF with boosted keyword matching**:

```python
# Adjusted RRF weights
{
    'keyword_weight': 0.6,  # Increased from 0.5
    'semantic_weight': 0.4,  # Decreased from 0.5
    'boost_exact_match': True,  # New: +50% score for exact title match
}
```

Re-ran experiment with 3 variants (days 8-14):
- Variant A: Control (BM25 only)
- Variant B: Original hybrid (50/50 RRF)
- Variant C: Adjusted hybrid (60/40 RRF + exact-match boost)

**Final Results** (Day 14):

| Metric | Control (A) | Hybrid 50/50 (B) | Hybrid 60/40 (C) | Winner |
|--------|-------------|------------------|------------------|--------|
| **CTR@10** | 42.5% | 46.8% (+10.1%) | 48.2% (+13.4%) | **C** |
| **Exact-match CTR** | 68.2% | 64.1% (-6.0%) | 70.1% (+2.8%) | **C** |
| **Conceptual CTR** | 28.4% | 38.7% (+36.3%) | 37.9% (+33.5%) | **B/C** |
| **Zero-result rate** | 2.5% | 1.8% (-28%) | 1.9% (-24%) | **B/C** |
| **Latency P95** | 287ms | 316ms (+10%) | 318ms (+11%) | **A** |
| **Abandonment** | 18.1% | 15.4% (-15%) | 14.8% (-18%) | **C** |

**Statistical Significance** (Variant C vs Control):
- CTR@10: p < 0.001 (highly significant)
- 95% CI for CTR difference: [+5.1pp, +6.3pp]
- Cohen's d = 0.28 (small-medium effect size)
- **Conclusion**: Variant C significantly improves overall CTR while maintaining exact-match quality

---

**Business Impact Analysis** (6 months post-rollout):

**Direct Search Metrics**:
- CTR@10: 42.5% â†’ 48.2% (+13.4%, sustained)
- Zero-result queries: 2.5% â†’ 1.7% (-32%)
- Mean click position: 3.24 â†’ 2.61 (-19%, users find answers faster)
- Session abandonment: 18.1% â†’ 14.3% (-21%)

**User Engagement**:
- Pages per session: 4.2 â†’ 5.8 (+38%, users explore more relevant content)
- Session duration: 6.8 min â†’ 8.4 min (+24%)
- Return visits (30-day): 68% â†’ 74% (+6pp)

**Business KPIs**:
- Support ticket submissions: 42,000/month â†’ 31,000/month (-26%, $165K/year savings at $15/ticket)
- User satisfaction score: 3.4/5 â†’ 4.1/5 (+21%)
- Premium subscription conversions: 2.8% â†’ 3.4% (+21%, $890K/year additional revenue)
- Documentation coverage gaps identified: 127 topics with high zero-result rates â†’ created new docs

**Financial ROI**:
- **Investment**: 
  - Engineering (6 weeks): $180K
  - Infrastructure (semantic indexing): $45K setup + $45K/month operational
  - Total Year 1: $180K + $45K + ($45K Ã— 12) = $765K

- **Returns (Year 1)**:
  - Support cost reduction: $165K/year
  - Premium conversion increase: $890K/year
  - Avoided failed experiment costs: ~$200K/year (estimated from historical pattern)
  - Total benefit: $1,255K/year

- **ROI**: ($1,255K - $765K) / $765K = **64% Year 1 ROI**
- **Payback period**: 7.3 months

**Cultural Impact**:
- Stakeholder confidence restored: "Now we have data, not opinions"
- 100% of search changes now require A/B testing
- Framework reused for 8 additional experiments in first year
- Engineering team morale improved: "We can prove our work matters"

---

### Key Learnings from TechDocs A/B Testing Journey

1. **Segment Analysis is Critical**: Aggregate metrics can hide degradations in important user segments (exact-match queries).

2. **Guardrail Metrics Prevent Regressions**: Zero-result rate and latency guardrails prevented shipping a variant that improved CTR but broke other aspects.

3. **A/A Testing Validates Framework**: The 3-day A/A test caught allocation bias issues before the real experiment.

4. **Multi-Variant Tests Enable Iteration**: Testing Variant C (adjusted hybrid) in the same experiment saved 2 weeks vs running sequential tests.

5. **Pre-Experiment Cost Validation**: Shadow traffic testing revealed $45K/month operational cost, enabling informed go/no-go decision.

6. **Statistical Rigor Builds Trust**: P-values, confidence intervals, and effect sizes convinced skeptical stakeholders better than anecdotal evidence.

7. **Automated Pipelines Scale**: Application Insights â†’ Cosmos DB â†’ Power BI pipeline enabled real-time monitoring for 8 subsequent experiments.

**Framework Reusability**: The same A/B testing infrastructure was reused for:
- Custom analyzer updates (2 experiments)
- Scoring profile changes (3 experiments)  
- Query suggestion algorithms (2 experiments)
- Facet ordering (1 experiment)

Total experiments run in Year 1: **9 experiments**, 7 shipped (78% success rate), 2 rejected based on data.

---

### A/B Testing Fundamentals

```python
class ABTestingOverview:
    """Understanding A/B testing for search."""
    
    @staticmethod
    def testing_framework():
        """
        A/B testing framework components:
        
        1. Hypothesis: What you're testing
        2. Variants: Control (A) vs Treatment (B)
        3. Randomization: User assignment to variants
        4. Metrics: Success criteria
        5. Sample size: Statistical power
        6. Duration: Test run time
        7. Analysis: Statistical significance
        """
        return {
            'hypothesis_examples': [
                'New scoring profile improves relevance',
                'Hybrid search outperforms full-text search',
                'Semantic ranking increases click-through rate',
                'Custom analyzer reduces zero-result queries'
            ],
            'variant_types': {
                'control': 'Baseline/current implementation',
                'treatment': 'New implementation being tested',
                'multi_variant': '2+ treatments (A/B/C/D testing)'
            },
            'key_metrics': {
                'relevance': ['NDCG@10', 'MRR', 'MAP'],
                'engagement': ['CTR', 'time_on_page', 'bounce_rate'],
                'business': ['conversion_rate', 'revenue_per_search', 'cart_adds']
            },
            'statistical_requirements': {
                'significance_level': 'Î± = 0.05 (95% confidence)',
                'power': 'Î² = 0.80 (80% power to detect effect)',
                'minimum_detectable_effect': 'MDE typically 2-5%'
            }
        }
    
    @staticmethod
    def experiment_lifecycle():
        """
        A/B test lifecycle stages.
        """
        return {
            '1_design': {
                'tasks': [
                    'Define hypothesis',
                    'Select metrics',
                    'Calculate sample size',
                    'Determine duration',
                    'Plan randomization'
                ],
                'duration': '1-3 days'
            },
            '2_implementation': {
                'tasks': [
                    'Build variants',
                    'Implement randomization',
                    'Setup tracking',
                    'Validate instrumentation',
                    'Run A/A test'
                ],
                'duration': '3-7 days'
            },
            '3_execution': {
                'tasks': [
                    'Launch experiment',
                    'Monitor metrics',
                    'Check for issues',
                    'Ensure balanced traffic'
                ],
                'duration': '1-4 weeks'
            },
            '4_analysis': {
                'tasks': [
                    'Statistical testing',
                    'Segment analysis',
                    'Validate results',
                    'Document findings'
                ],
                'duration': '1-2 days'
            },
            '5_decision': {
                'tasks': [
                    'Review results',
                    'Make decision (ship/iterate/abandon)',
                    'Communicate results',
                    'Plan rollout if shipping'
                ],
                'duration': '1 day'
            }
        }

# Usage
overview = ABTestingOverview()

framework = overview.testing_framework()
print("A/B Testing Framework:")
print("\nHypothesis Examples:")
for hypothesis in framework['hypothesis_examples']:
    print(f"  â€¢ {hypothesis}")

print("\nKey Metrics:")
for category, metrics in framework['key_metrics'].items():
    print(f"  {category}: {', '.join(metrics)}")

lifecycle = overview.experiment_lifecycle()
print("\n\nExperiment Lifecycle:")
for stage, details in lifecycle.items():
    print(f"\n{stage}:")
    print(f"  Duration: {details['duration']}")
    print(f"  Tasks: {len(details['tasks'])}")
```

---

## Experimental Design

### Sample Size Calculation

```python
import math
from scipy import stats

class SampleSizeCalculator:
    """Calculate required sample size for A/B tests."""
    
    @staticmethod
    def calculate_for_proportions(p1: float, p2: float, 
                                   alpha: float = 0.05,
                                   power: float = 0.80) -> int:
        """
        Calculate sample size for proportion metrics (CTR, conversion rate).
        
        Args:
            p1: Baseline proportion (e.g., 0.25 for 25% CTR)
            p2: Expected proportion in treatment
            alpha: Significance level (default 0.05)
            power: Statistical power (default 0.80)
        
        Returns:
            Required sample size per variant
        """
        # Z-scores
        z_alpha = stats.norm.ppf(1 - alpha/2)  # Two-tailed
        z_beta = stats.norm.ppf(power)
        
        # Pooled proportion
        p_pooled = (p1 + p2) / 2
        
        # Sample size calculation
        numerator = (z_alpha + z_beta) ** 2 * 2 * p_pooled * (1 - p_pooled)
        denominator = (p2 - p1) ** 2
        
        n = math.ceil(numerator / denominator)
        return n
    
    @staticmethod
    def calculate_for_means(mean1: float, mean2: float, 
                           std: float,
                           alpha: float = 0.05,
                           power: float = 0.80) -> int:
        """
        Calculate sample size for continuous metrics (NDCG, MRR).
        
        Args:
            mean1: Baseline mean
            mean2: Expected mean in treatment
            std: Standard deviation
            alpha: Significance level
            power: Statistical power
        
        Returns:
            Required sample size per variant
        """
        z_alpha = stats.norm.ppf(1 - alpha/2)
        z_beta = stats.norm.ppf(power)
        
        effect_size = abs(mean2 - mean1) / std
        
        n = math.ceil(2 * ((z_alpha + z_beta) / effect_size) ** 2)
        return n
    
    @staticmethod
    def calculate_test_duration(sample_size: int, 
                               daily_users: int,
                               allocation_percent: float = 0.5) -> int:
        """
        Calculate test duration in days.
        
        Args:
            sample_size: Required sample size per variant
            daily_users: Average daily users
            allocation_percent: Percentage allocated to each variant (0.5 = 50%)
        
        Returns:
            Duration in days
        """
        daily_per_variant = daily_users * allocation_percent
        days = math.ceil(sample_size / daily_per_variant)
        return days

# Usage
calculator = SampleSizeCalculator()

# Example 1: CTR improvement (25% â†’ 27%)
sample_size_ctr = calculator.calculate_for_proportions(
    p1=0.25,  # Current 25% CTR
    p2=0.27,  # Target 27% CTR (8% relative increase)
    alpha=0.05,
    power=0.80
)

print("Sample Size for CTR Test:")
print(f"  Baseline CTR: 25%")
print(f"  Target CTR: 27%")
print(f"  Sample size per variant: {sample_size_ctr:,}")
print(f"  Total sample size: {sample_size_ctr * 2:,}")

# Example 2: NDCG improvement
sample_size_ndcg = calculator.calculate_for_means(
    mean1=0.75,  # Current NDCG@10
    mean2=0.78,  # Target NDCG@10 (4% relative increase)
    std=0.15,    # Estimated standard deviation
    alpha=0.05,
    power=0.80
)

print(f"\nSample Size for NDCG Test:")
print(f"  Baseline NDCG@10: 0.75")
print(f"  Target NDCG@10: 0.78")
print(f"  Sample size per variant: {sample_size_ndcg:,}")

# Example 3: Test duration
duration = calculator.calculate_test_duration(
    sample_size=sample_size_ctr,
    daily_users=10000,
    allocation_percent=0.5
)

print(f"\nTest Duration:")
print(f"  Daily users: 10,000")
print(f"  Allocation: 50% per variant")
print(f"  Duration: {duration} days")
```

### Randomization Strategy

```python
import hashlib
import random

class Randomizer:
    """Handle user randomization for A/B tests."""
    
    def __init__(self, experiment_id: str, variants: list, 
                 allocations: list = None):
        """
        Initialize randomizer.
        
        Args:
            experiment_id: Unique experiment identifier
            variants: List of variant names (e.g., ['control', 'treatment'])
            allocations: List of allocation percentages (e.g., [0.5, 0.5])
                        If None, defaults to equal allocation
        """
        self.experiment_id = experiment_id
        self.variants = variants
        
        if allocations is None:
            self.allocations = [1.0 / len(variants)] * len(variants)
        else:
            if abs(sum(allocations) - 1.0) > 0.001:
                raise ValueError("Allocations must sum to 1.0")
            self.allocations = allocations
        
        # Calculate cumulative allocations for bucket assignment
        self.cumulative_allocations = []
        cumsum = 0
        for alloc in self.allocations:
            cumsum += alloc
            self.cumulative_allocations.append(cumsum)
    
    def assign_variant(self, user_id: str) -> str:
        """
        Assign user to variant using consistent hashing.
        
        Same user_id always gets same variant for this experiment.
        """
        # Create hash of user_id + experiment_id
        hash_input = f"{user_id}:{self.experiment_id}"
        hash_value = hashlib.md5(hash_input.encode()).hexdigest()
        
        # Convert to number between 0 and 1
        hash_int = int(hash_value, 16)
        bucket = (hash_int % 10000) / 10000.0
        
        # Assign to variant based on bucket
        for i, cumsum in enumerate(self.cumulative_allocations):
            if bucket < cumsum:
                return self.variants[i]
        
        return self.variants[-1]
    
    def get_allocation_stats(self, user_ids: list) -> dict:
        """
        Get actual allocation statistics for a list of users.
        """
        variant_counts = {v: 0 for v in self.variants}
        
        for user_id in user_ids:
            variant = self.assign_variant(user_id)
            variant_counts[variant] += 1
        
        total = len(user_ids)
        return {
            'counts': variant_counts,
            'percentages': {
                v: count / total * 100 
                for v, count in variant_counts.items()
            },
            'expected': {
                v: self.allocations[i] * 100
                for i, v in enumerate(self.variants)
            }
        }

# Usage
randomizer = Randomizer(
    experiment_id="scoring_profile_test_001",
    variants=['control', 'treatment'],
    allocations=[0.5, 0.5]
)

# Assign users
user1_variant = randomizer.assign_variant("user123")
user2_variant = randomizer.assign_variant("user456")

print(f"User assignments:")
print(f"  user123 â†’ {user1_variant}")
print(f"  user456 â†’ {user2_variant}")

# Verify consistency
print(f"\nConsistency check (same user, same variant):")
print(f"  user123 â†’ {randomizer.assign_variant('user123')}")
print(f"  user123 â†’ {randomizer.assign_variant('user123')}")

# Check allocation balance
test_users = [f"user{i}" for i in range(10000)]
stats = randomizer.get_allocation_stats(test_users)

print(f"\nAllocation Statistics (10,000 users):")
for variant in randomizer.variants:
    expected = stats['expected'][variant]
    actual = stats['percentages'][variant]
    print(f"  {variant}: {actual:.2f}% (expected {expected:.0f}%)")
```

---

## Implementation

### Experiment Tracking

```python
from dataclasses import dataclass, asdict
from typing import Dict, Any, Optional
from datetime import datetime
import json

@dataclass
class ExperimentConfig:
    """Experiment configuration."""
    experiment_id: str
    name: str
    hypothesis: str
    variants: list
    allocations: list
    metrics: list
    start_date: str
    end_date: Optional[str]
    status: str  # draft, running, completed, stopped
    metadata: Dict[str, Any]
    
    def to_dict(self):
        return asdict(self)

@dataclass
class ExperimentEvent:
    """Single experiment event (query, click, etc.)."""
    event_id: str
    experiment_id: str
    user_id: str
    variant: str
    timestamp: str
    event_type: str  # query, click, conversion, etc.
    metrics: Dict[str, Any]
    
    def to_dict(self):
        return asdict(self)

class ExperimentTracker:
    """Track A/B test experiments and events."""
    
    def __init__(self, storage_path: str = "experiments"):
        self.storage_path = storage_path
        import os
        os.makedirs(storage_path, exist_ok=True)
    
    def create_experiment(self, config: ExperimentConfig):
        """Create new experiment."""
        # Save configuration
        config_path = f"{self.storage_path}/{config.experiment_id}_config.json"
        with open(config_path, 'w') as f:
            json.dump(config.to_dict(), f, indent=2)
        
        print(f"Created experiment: {config.name}")
        print(f"  ID: {config.experiment_id}")
        print(f"  Variants: {config.variants}")
        print(f"  Allocations: {config.allocations}")
        
        return config
    
    def log_event(self, event: ExperimentEvent):
        """Log experiment event."""
        events_path = f"{self.storage_path}/{event.experiment_id}_events.jsonl"
        
        with open(events_path, 'a') as f:
            f.write(json.dumps(event.to_dict()) + '\n')
    
    def get_experiment_config(self, experiment_id: str) -> ExperimentConfig:
        """Load experiment configuration."""
        config_path = f"{self.storage_path}/{experiment_id}_config.json"
        
        with open(config_path, 'r') as f:
            data = json.load(f)
            return ExperimentConfig(**data)
    
    def get_events(self, experiment_id: str) -> list:
        """Load all events for an experiment."""
        events_path = f"{self.storage_path}/{experiment_id}_events.jsonl"
        events = []
        
        try:
            with open(events_path, 'r') as f:
                for line in f:
                    event_data = json.loads(line)
                    events.append(ExperimentEvent(**event_data))
        except FileNotFoundError:
            pass
        
        return events

# Usage
tracker = ExperimentTracker()

# Create experiment
experiment = ExperimentConfig(
    experiment_id="exp_semantic_001",
    name="Semantic Search vs BM25",
    hypothesis="Semantic search improves NDCG@10 by 5%",
    variants=["control_bm25", "treatment_semantic"],
    allocations=[0.5, 0.5],
    metrics=["ndcg@10", "mrr", "ctr"],
    start_date=datetime.now().isoformat(),
    end_date=None,
    status="running",
    metadata={
        "created_by": "data_team",
        "min_sample_size": 5000
    }
)

tracker.create_experiment(experiment)

# Log events
event1 = ExperimentEvent(
    event_id="evt_001",
    experiment_id="exp_semantic_001",
    user_id="user123",
    variant="control_bm25",
    timestamp=datetime.now().isoformat(),
    event_type="query",
    metrics={
        "ndcg@10": 0.78,
        "mrr": 0.85,
        "query_text": "machine learning tutorial"
    }
)

tracker.log_event(event1)
print(f"\nLogged event: {event1.event_type} for {event1.variant}")
```

### Search Variant Execution

```python
from azure.search.documents import SearchClient
from azure.core.credentials import AzureKeyCredential

class ABTestSearchExecutor:
    """Execute search queries with A/B variant logic."""
    
    def __init__(self, search_endpoint: str, index_name: str, 
                 search_key: str, randomizer: Randomizer):
        self.search_client = SearchClient(
            endpoint=search_endpoint,
            index_name=index_name,
            credential=AzureKeyCredential(search_key)
        )
        self.randomizer = randomizer
    
    def execute_search(self, user_id: str, query_text: str, 
                      variant_configs: dict, top: int = 10) -> dict:
        """
        Execute search with appropriate variant configuration.
        
        Args:
            user_id: User identifier for variant assignment
            query_text: Search query
            variant_configs: Dict mapping variant names to search parameters
            top: Number of results
        
        Returns:
            Search results with variant information
        """
        # Assign variant
        variant = self.randomizer.assign_variant(user_id)
        
        # Get variant configuration
        config = variant_configs.get(variant, {})
        
        # Execute search with variant parameters
        results = self.search_client.search(
            search_text=query_text,
            top=top,
            **config
        )
        
        # Collect results
        documents = []
        for result in results:
            documents.append({
                'id': result.get('id'),
                'score': result.get('@search.score'),
                'title': result.get('title')
            })
        
        return {
            'variant': variant,
            'query': query_text,
            'results': documents,
            'result_count': len(documents)
        }

# Usage
import os

# Setup A/B test variants
variant_configs = {
    'control_bm25': {
        # Standard BM25 search
        'scoring_profile': None,
        'query_type': 'simple'
    },
    'treatment_semantic': {
        # Semantic search
        'query_type': 'semantic',
        'semantic_configuration_name': 'my-semantic-config'
    }
}

executor = ABTestSearchExecutor(
    search_endpoint=os.getenv("SEARCH_ENDPOINT"),
    index_name="products",
    search_key=os.getenv("SEARCH_API_KEY"),
    randomizer=randomizer
)

# Execute search for user
# result = executor.execute_search(
#     user_id="user123",
#     query_text="laptop",
#     variant_configs=variant_configs,
#     top=10
# )
# 
# print(f"Variant: {result['variant']}")
# print(f"Results: {result['result_count']}")
```

---

## Statistical Analysis

### Hypothesis Testing

```python
from scipy import stats
import numpy as np

class HypothesisTester:
    """Perform statistical hypothesis tests."""
    
    @staticmethod
    def two_sample_t_test(control_values: list, treatment_values: list,
                         alpha: float = 0.05) -> dict:
        """
        Perform two-sample t-test for continuous metrics.
        
        Returns p-value and confidence interval.
        """
        # Perform t-test
        t_statistic, p_value = stats.ttest_ind(control_values, treatment_values)
        
        # Calculate means and standard deviations
        control_mean = np.mean(control_values)
        treatment_mean = np.mean(treatment_values)
        
        # Effect size (Cohen's d)
        pooled_std = np.sqrt(
            (np.var(control_values) + np.var(treatment_values)) / 2
        )
        cohens_d = (treatment_mean - control_mean) / pooled_std
        
        # Confidence interval for difference
        diff = treatment_mean - control_mean
        se_diff = np.sqrt(
            np.var(control_values) / len(control_values) +
            np.var(treatment_values) / len(treatment_values)
        )
        ci_lower = diff - 1.96 * se_diff
        ci_upper = diff + 1.96 * se_diff
        
        return {
            'control_mean': control_mean,
            'treatment_mean': treatment_mean,
            'difference': diff,
            'relative_change': (diff / control_mean * 100) if control_mean != 0 else 0,
            't_statistic': t_statistic,
            'p_value': p_value,
            'significant': p_value < alpha,
            'cohens_d': cohens_d,
            'confidence_interval': (ci_lower, ci_upper)
        }
    
    @staticmethod
    def two_proportion_z_test(control_successes: int, control_total: int,
                              treatment_successes: int, treatment_total: int,
                              alpha: float = 0.05) -> dict:
        """
        Perform two-proportion z-test for binary metrics (CTR, conversion).
        """
        # Calculate proportions
        p1 = control_successes / control_total
        p2 = treatment_successes / treatment_total
        
        # Pooled proportion
        p_pooled = (control_successes + treatment_successes) / (control_total + treatment_total)
        
        # Standard error
        se = np.sqrt(p_pooled * (1 - p_pooled) * (1/control_total + 1/treatment_total))
        
        # Z-statistic
        z_stat = (p2 - p1) / se
        
        # P-value (two-tailed)
        p_value = 2 * (1 - stats.norm.cdf(abs(z_stat)))
        
        # Confidence interval
        se_diff = np.sqrt(p1 * (1-p1) / control_total + p2 * (1-p2) / treatment_total)
        diff = p2 - p1
        ci_lower = diff - 1.96 * se_diff
        ci_upper = diff + 1.96 * se_diff
        
        return {
            'control_rate': p1,
            'treatment_rate': p2,
            'difference': diff,
            'relative_change': (diff / p1 * 100) if p1 != 0 else 0,
            'z_statistic': z_stat,
            'p_value': p_value,
            'significant': p_value < alpha,
            'confidence_interval': (ci_lower, ci_upper)
        }

# Usage
tester = HypothesisTester()

# Example 1: Continuous metric (NDCG@10)
control_ndcg = [0.75, 0.78, 0.72, 0.80, 0.76, 0.74, 0.79, 0.77, 0.73, 0.78]
treatment_ndcg = [0.78, 0.81, 0.79, 0.83, 0.80, 0.77, 0.82, 0.79, 0.80, 0.81]

t_test_result = tester.two_sample_t_test(control_ndcg, treatment_ndcg)

print("T-Test Results (NDCG@10):")
print(f"  Control mean: {t_test_result['control_mean']:.4f}")
print(f"  Treatment mean: {t_test_result['treatment_mean']:.4f}")
print(f"  Difference: {t_test_result['difference']:.4f} ({t_test_result['relative_change']:.2f}%)")
print(f"  P-value: {t_test_result['p_value']:.4f}")
print(f"  Significant: {t_test_result['significant']}")
print(f"  95% CI: ({t_test_result['confidence_interval'][0]:.4f}, {t_test_result['confidence_interval'][1]:.4f})")

# Example 2: Proportion metric (CTR)
z_test_result = tester.two_proportion_z_test(
    control_successes=250,    # 250 clicks
    control_total=1000,       # 1000 searches
    treatment_successes=285,  # 285 clicks
    treatment_total=1000,     # 1000 searches
    alpha=0.05
)

print("\n\nZ-Test Results (CTR):")
print(f"  Control CTR: {z_test_result['control_rate']:.2%}")
print(f"  Treatment CTR: {z_test_result['treatment_rate']:.2%}")
print(f"  Difference: {z_test_result['difference']:.2%} ({z_test_result['relative_change']:.2f}%)")
print(f"  P-value: {z_test_result['p_value']:.4f}")
print(f"  Significant: {z_test_result['significant']}")
```

---

## Azure Integration

### Cosmos DB Storage

```python
from azure.cosmos import CosmosClient, PartitionKey

class CosmosDBExperimentStore:
    """Store experiment data in Cosmos DB."""
    
    def __init__(self, endpoint: str, key: str, 
                 database_name: str = "experiments",
                 container_name: str = "ab_tests"):
        self.client = CosmosClient(endpoint, key)
        
        # Create database if not exists
        self.database = self.client.create_database_if_not_exists(database_name)
        
        # Create container if not exists
        self.container = self.database.create_container_if_not_exists(
            id=container_name,
            partition_key=PartitionKey(path="/experiment_id")
        )
    
    def store_event(self, event: ExperimentEvent):
        """Store experiment event in Cosmos DB."""
        self.container.upsert_item(event.to_dict())
    
    def query_events(self, experiment_id: str, variant: str = None) -> list:
        """Query events for an experiment."""
        query = f"SELECT * FROM c WHERE c.experiment_id = '{experiment_id}'"
        
        if variant:
            query += f" AND c.variant = '{variant}'"
        
        items = list(self.container.query_items(
            query=query,
            enable_cross_partition_query=True
        ))
        
        return items
    
    def get_variant_metrics(self, experiment_id: str) -> dict:
        """
        Aggregate metrics by variant.
        """
        query = f"""
        SELECT 
            c.variant,
            COUNT(1) as event_count,
            AVG(c.metrics.ndcg_at_10) as avg_ndcg,
            AVG(c.metrics.mrr) as avg_mrr
        FROM c 
        WHERE c.experiment_id = '{experiment_id}'
        GROUP BY c.variant
        """
        
        results = list(self.container.query_items(
            query=query,
            enable_cross_partition_query=True
        ))
        
        return {r['variant']: r for r in results}

# Usage
# cosmos_store = CosmosDBExperimentStore(
#     endpoint=os.getenv("COSMOS_ENDPOINT"),
#     key=os.getenv("COSMOS_KEY")
# )
# 
# # Store event
# cosmos_store.store_event(event1)
# 
# # Query events
# events = cosmos_store.query_events("exp_semantic_001", variant="control_bm25")
# print(f"Events for control: {len(events)}")
# 
# # Get aggregated metrics
# metrics = cosmos_store.get_variant_metrics("exp_semantic_001")
# for variant, stats in metrics.items():
#     print(f"{variant}: {stats['event_count']} events, avg NDCG = {stats['avg_ndcg']:.4f}")
```

---

## Results Tracking

### Results Dashboard

```python
class ExperimentDashboard:
    """Generate experiment results dashboard."""
    
    def __init__(self, tracker: ExperimentTracker):
        self.tracker = tracker
    
    def generate_summary(self, experiment_id: str) -> dict:
        """Generate experiment summary."""
        config = self.tracker.get_experiment_config(experiment_id)
        events = self.tracker.get_events(experiment_id)
        
        # Group by variant
        variant_events = {}
        for event in events:
            if event.variant not in variant_events:
                variant_events[event.variant] = []
            variant_events[event.variant].append(event)
        
        # Calculate metrics per variant
        variant_stats = {}
        for variant, events_list in variant_events.items():
            ndcg_values = [e.metrics.get('ndcg@10') for e in events_list if 'ndcg@10' in e.metrics]
            mrr_values = [e.metrics.get('mrr') for e in events_list if 'mrr' in e.metrics]
            
            variant_stats[variant] = {
                'event_count': len(events_list),
                'avg_ndcg': np.mean(ndcg_values) if ndcg_values else 0,
                'avg_mrr': np.mean(mrr_values) if mrr_values else 0
            }
        
        return {
            'experiment': config.name,
            'status': config.status,
            'total_events': len(events),
            'variants': variant_stats
        }
    
    def print_summary(self, experiment_id: str):
        """Print formatted experiment summary."""
        summary = self.generate_summary(experiment_id)
        
        print(f"Experiment: {summary['experiment']}")
        print(f"Status: {summary['status']}")
        print(f"Total Events: {summary['total_events']}")
        print(f"\nVariant Results:")
        
        for variant, stats in summary['variants'].items():
            print(f"\n  {variant}:")
            print(f"    Events: {stats['event_count']}")
            print(f"    Avg NDCG@10: {stats['avg_ndcg']:.4f}")
            print(f"    Avg MRR: {stats['avg_mrr']:.4f}")

# Usage
# dashboard = ExperimentDashboard(tracker)
# dashboard.print_summary("exp_semantic_001")
```

---

## Analysis Tools

### Complete Analysis Pipeline

```python
class ExperimentAnalyzer:
    """Complete A/B test analysis pipeline."""
    
    def __init__(self, experiment_id: str, tracker: ExperimentTracker):
        self.experiment_id = experiment_id
        self.tracker = tracker
        self.config = tracker.get_experiment_config(experiment_id)
        self.events = tracker.get_events(experiment_id)
    
    def run_analysis(self) -> dict:
        """Run complete analysis."""
        results = {
            'experiment_id': self.experiment_id,
            'config': self.config.to_dict(),
            'sample_sizes': self._calculate_sample_sizes(),
            'metrics': self._analyze_metrics(),
            'statistical_tests': self._run_statistical_tests(),
            'recommendation': self._make_recommendation()
        }
        
        return results
    
    def _calculate_sample_sizes(self) -> dict:
        """Calculate actual sample sizes per variant."""
        variant_counts = {}
        for event in self.events:
            variant = event.variant
            variant_counts[variant] = variant_counts.get(variant, 0) + 1
        
        return variant_counts
    
    def _analyze_metrics(self) -> dict:
        """Analyze metrics by variant."""
        variant_metrics = {}
        
        for variant in self.config.variants:
            variant_events = [e for e in self.events if e.variant == variant]
            
            metrics = {}
            for metric_name in self.config.metrics:
                values = [
                    e.metrics.get(metric_name)
                    for e in variant_events
                    if metric_name in e.metrics and e.metrics[metric_name] is not None
                ]
                
                if values:
                    metrics[metric_name] = {
                        'mean': np.mean(values),
                        'std': np.std(values),
                        'min': np.min(values),
                        'max': np.max(values),
                        'count': len(values)
                    }
            
            variant_metrics[variant] = metrics
        
        return variant_metrics
    
    def _run_statistical_tests(self) -> dict:
        """Run statistical tests comparing variants."""
        if len(self.config.variants) != 2:
            return {'error': 'Statistical tests require exactly 2 variants'}
        
        control = self.config.variants[0]
        treatment = self.config.variants[1]
        
        control_events = [e for e in self.events if e.variant == control]
        treatment_events = [e for e in self.events if e.variant == treatment]
        
        test_results = {}
        tester = HypothesisTester()
        
        for metric_name in self.config.metrics:
            control_values = [
                e.metrics.get(metric_name)
                for e in control_events
                if metric_name in e.metrics and e.metrics[metric_name] is not None
            ]
            
            treatment_values = [
                e.metrics.get(metric_name)
                for e in treatment_events
                if metric_name in e.metrics and e.metrics[metric_name] is not None
            ]
            
            if control_values and treatment_values:
                result = tester.two_sample_t_test(control_values, treatment_values)
                test_results[metric_name] = result
        
        return test_results
    
    def _make_recommendation(self) -> dict:
        """Make recommendation based on results."""
        tests = self._run_statistical_tests()
        
        if not tests:
            return {'decision': 'insufficient_data'}
        
        # Check if primary metric is significant
        primary_metric = self.config.metrics[0] if self.config.metrics else None
        
        if primary_metric and primary_metric in tests:
            result = tests[primary_metric]
            
            if result['significant'] and result['difference'] > 0:
                return {
                    'decision': 'ship_treatment',
                    'reason': f"{primary_metric} significantly improved",
                    'improvement': f"{result['relative_change']:.2f}%"
                }
            elif result['significant'] and result['difference'] < 0:
                return {
                    'decision': 'keep_control',
                    'reason': f"{primary_metric} significantly degraded",
                    'degradation': f"{result['relative_change']:.2f}%"
                }
            else:
                return {
                    'decision': 'no_significant_difference',
                    'reason': f"No significant change in {primary_metric}"
                }
        
        return {'decision': 'manual_review_required'}
    
    def print_report(self):
        """Print formatted analysis report."""
        analysis = self.run_analysis()
        
        print("=" * 60)
        print(f"A/B Test Analysis Report")
        print("=" * 60)
        print(f"\nExperiment: {self.config.name}")
        print(f"Hypothesis: {self.config.hypothesis}")
        print(f"Status: {self.config.status}")
        
        print(f"\nSample Sizes:")
        for variant, count in analysis['sample_sizes'].items():
            print(f"  {variant}: {count:,}")
        
        print(f"\nMetrics by Variant:")
        for variant, metrics in analysis['metrics'].items():
            print(f"\n  {variant}:")
            for metric_name, stats in metrics.items():
                print(f"    {metric_name}: {stats['mean']:.4f} Â± {stats['std']:.4f}")
        
        print(f"\nStatistical Tests:")
        for metric_name, result in analysis['statistical_tests'].items():
            print(f"\n  {metric_name}:")
            print(f"    Difference: {result['difference']:.4f} ({result['relative_change']:.2f}%)")
            print(f"    P-value: {result['p_value']:.4f}")
            print(f"    Significant: {'âœ“ YES' if result['significant'] else 'âœ— NO'}")
            print(f"    95% CI: ({result['confidence_interval'][0]:.4f}, {result['confidence_interval'][1]:.4f})")
        
        print(f"\nRecommendation:")
        rec = analysis['recommendation']
        print(f"  Decision: {rec['decision'].replace('_', ' ').title()}")
        if 'reason' in rec:
            print(f"  Reason: {rec['reason']}")

# Usage
# analyzer = ExperimentAnalyzer("exp_semantic_001", tracker)
# analyzer.print_report()
```

---

## Best Practices

### 1. Always Run A/A Tests First

**Why It Matters**

An A/A test (both variants identical) validates your randomization, instrumentation, and analysis pipeline. If you detect a "significant difference" when there should be none, your framework has a bug.

**Implementation**

```python
class AATestValidator:
    """Validate A/B testing framework with A/A test."""
    
    def __init__(self, randomizer: Randomizer):
        self.randomizer = randomizer
    
    def run_aa_test(self, users: list, metric_function, alpha: float = 0.05):
        """
        Run A/A test to validate framework.
        
        Args:
            users: List of user IDs
            metric_function: Function that returns metric value for a user
            alpha: Significance level
        
        Returns:
            Validation results
        """
        # Assign users to "variants" (both identical)
        variant_a_users = []
        variant_b_users = []
        
        for user in users:
            variant = self.randomizer.assign_variant(user)
            if variant == self.randomizer.variants[0]:
                variant_a_users.append(user)
            else:
                variant_b_users.append(user)
        
        # Collect metrics
        variant_a_metrics = [metric_function(user) for user in variant_a_users]
        variant_b_metrics = [metric_function(user) for user in variant_b_users]
        
        # Statistical test
        tester = HypothesisTester()
        result = tester.two_sample_t_test(variant_a_metrics, variant_b_metrics, alpha)
        
        # Validate
        allocation_balance = abs(len(variant_a_users) / len(users) - 0.5)
        
        validation_result = {
            'allocation_balance': {
                'variant_a_pct': len(variant_a_users) / len(users) * 100,
                'variant_b_pct': len(variant_b_users) / len(users) * 100,
                'imbalance': allocation_balance * 100,
                'acceptable': allocation_balance < 0.02  # Within 2%
            },
            'statistical_test': {
                'p_value': result['p_value'],
                'significant': result['significant'],
                'should_not_be_significant': not result['significant']
            },
            'validation_status': 'PASS' if (not result['significant'] and allocation_balance < 0.02) else 'FAIL'
        }
        
        return validation_result
    
    def print_validation_report(self, validation_result):
        """Print A/A test validation report."""
        print("A/A Test Validation Report")
        print("=" * 50)
        
        # Allocation balance
        alloc = validation_result['allocation_balance']
        balance_status = "âœ“ PASS" if alloc['acceptable'] else "âœ— FAIL"
        print(f"\nAllocation Balance: {balance_status}")
        print(f"  Variant A: {alloc['variant_a_pct']:.2f}%")
        print(f"  Variant B: {alloc['variant_b_pct']:.2f}%")
        print(f"  Imbalance: {alloc['imbalance']:.2f}% (target <2%)")
        
        # Statistical test
        stat = validation_result['statistical_test']
        stat_status = "âœ“ PASS" if stat['should_not_be_significant'] else "âœ— FAIL"
        print(f"\nStatistical Test: {stat_status}")
        print(f"  P-value: {stat['p_value']:.4f}")
        print(f"  Significant: {stat['significant']} (should be False)")
        
        # Overall
        overall_status = validation_result['validation_status']
        print(f"\n{'='*50}")
        print(f"Overall Status: {overall_status}")
        
        if overall_status == 'FAIL':
            print("\nâš  WARNING: A/A test failed. Do NOT proceed with A/B test.")
            print("Investigate randomization or instrumentation issues.")

# Usage
randomizer = Randomizer(
    experiment_id="aa_test_validation",
    variants=['variant_a', 'variant_b'],
    allocations=[0.5, 0.5]
)

validator = AATestValidator(randomizer)

# Simulate users and metric
test_users = [f"user{i}" for i in range(10000)]

# Metric function (should return same distribution for both variants in A/A test)
import random
def simulated_metric(user_id):
    random.seed(hash(user_id))  # Deterministic per user
    return random.gauss(0.75, 0.1)  # Mean=0.75, SD=0.1

# Run A/A test
aa_result = validator.run_aa_test(test_users, simulated_metric)
validator.print_validation_report(aa_result)
```

**Expected A/A Test Results**:
- Allocation: 48-52% per variant (acceptable imbalance <2%)
- P-value: >0.05 (no significant difference)
- **If A/A test fails**: Fix framework before running real A/B test

---

### 2. Calculate Statistical Power Before Starting

**Avoid Underpowered Experiments**

Running an experiment without sufficient sample size wastes time and risks false negatives (missing real improvements).

**Power Analysis Framework**

```python
class PowerAnalysis:
    """Analyze statistical power for experiment planning."""
    
    @staticmethod
    def calculate_required_sample_size(baseline: float, mde: float, 
                                       alpha: float = 0.05, 
                                       power: float = 0.80,
                                       metric_type: str = 'proportion') -> dict:
        """
        Calculate required sample size for desired power.
        
        Args:
            baseline: Baseline metric value (e.g., 0.30 for 30% CTR)
            mde: Minimum Detectable Effect (e.g., 0.02 for 2 percentage points)
            alpha: Significance level (default 0.05)
            power: Statistical power (default 0.80)
            metric_type: 'proportion' or 'continuous'
        
        Returns:
            Sample size requirements and test duration estimates
        """
        if metric_type == 'proportion':
            treatment_value = baseline + mde
            calculator = SampleSizeCalculator()
            n_per_variant = calculator.calculate_for_proportions(baseline, treatment_value, alpha, power)
        else:
            # For continuous metrics, need std dev estimate
            # Assume CV (coefficient of variation) = 0.5 as default
            std_estimate = baseline * 0.5
            treatment_value = baseline + mde
            calculator = SampleSizeCalculator()
            n_per_variant = calculator.calculate_for_means(baseline, treatment_value, std_estimate, alpha, power)
        
        return {
            'baseline': baseline,
            'mde': mde,
            'mde_relative': (mde / baseline * 100) if baseline > 0 else 0,
            'alpha': alpha,
            'power': power,
            'n_per_variant': n_per_variant,
            'n_total': n_per_variant * 2
        }
    
    @staticmethod
    def estimate_test_duration(n_per_variant: int, daily_traffic: int,
                              allocation: float = 0.5) -> dict:
        """
        Estimate test duration based on traffic.
        
        Returns recommended duration accounting for:
        - Sample size requirements
        - Weekly seasonality (minimum 7 days)
        - Buffer for data quality issues
        """
        daily_per_variant = daily_traffic * allocation
        min_days_for_sample = math.ceil(n_per_variant / daily_per_variant)
        
        # Apply constraints
        recommended_days = max(min_days_for_sample, 7)  # Minimum 1 week
        
        # Add buffer for weekends (if needed for full week)
        if recommended_days < 14 and min_days_for_sample > 7:
            recommended_days = 14  # 2 weeks for cleaner weekly pattern
        
        return {
            'min_days_for_sample': min_days_for_sample,
            'recommended_days': recommended_days,
            'daily_per_variant': int(daily_per_variant),
            'expected_total_sample': int(recommended_days * daily_per_variant),
            'buffer_pct': ((recommended_days * daily_per_variant) - n_per_variant) / n_per_variant * 100
        }
    
    @staticmethod
    def print_power_analysis(baseline: float, mde_options: list, 
                            daily_traffic: int, metric_name: str = "CTR"):
        """
        Print power analysis table for different MDE values.
        """
        print(f"Power Analysis for {metric_name}")
        print(f"Baseline: {baseline:.2%}")
        print(f"Daily Traffic: {daily_traffic:,}")
        print(f"Allocation: 50/50")
        print(f"\n{'MDE':<10} {'Relative':<12} {'Sample/Var':<15} {'Total':<12} {'Days'}")
        print("-" * 65)
        
        for mde in mde_options:
            result = PowerAnalysis.calculate_required_sample_size(baseline, mde, metric_type='proportion')
            duration = PowerAnalysis.estimate_test_duration(result['n_per_variant'], daily_traffic)
            
            print(f"{mde:>6.1%}    {result['mde_relative']:>6.1f}%      "
                  f"{result['n_per_variant']:>10,}    {result['n_total']:>10,}   "
                  f"{duration['recommended_days']:>2} days")

# Usage
PowerAnalysis.print_power_analysis(
    baseline=0.425,  # 42.5% CTR
    mde_options=[0.005, 0.01, 0.02, 0.03],  # 0.5%, 1%, 2%, 3% absolute change
    daily_traffic=420000,
    metric_name="CTR@10"
)
```

**Output Example**:
```
Power Analysis for CTR@10
Baseline: 42.50%
Daily Traffic: 420,000
Allocation: 50/50

MDE         Relative     Sample/Var      Total        Days
-----------------------------------------------------------------
  0.5%         1.2%          396,542      793,084     2 days
  1.0%         2.4%           99,136      198,272     7 days
  2.0%         4.7%           24,784       49,568     7 days
  3.0%         7.1%           11,015       22,030     7 days
```

**Interpretation**:
- Smaller MDE (0.5%) requires massive sample size (396K per variant)
- 2-3% MDE is realistic for 1-2 week experiments
- Always run at least 7 days to capture weekly patterns

---

### 3. Use Stratified Sampling for Segment Analysis

**Why Segment Analysis Matters**

Aggregate metrics can mask critical differences across user segments. A "winning" variant might degrade experience for important user groups.

**Implementation**

```python
class SegmentAnalyzer:
    """Analyze A/B test results by user segments."""
    
    def __init__(self, events: list):
        """
        Args:
            events: List of ExperimentEvent objects
        """
        self.events = events
    
    def segment_by_attribute(self, segment_attr: str, metric_name: str) -> dict:
        """
        Segment analysis by user attribute.
        
        Args:
            segment_attr: Attribute to segment by (e.g., 'user_type', 'query_category')
            metric_name: Metric to analyze
        
        Returns:
            Results by segment and variant
        """
        from collections import defaultdict
        
        # Group by segment and variant
        segment_variant_metrics = defaultdict(lambda: defaultdict(list))
        
        for event in self.events:
            segment_value = event.metadata.get(segment_attr)
            if segment_value and metric_name in event.metrics:
                variant = event.variant
                metric_value = event.metrics[metric_name]
                segment_variant_metrics[segment_value][variant].append(metric_value)
        
        # Analyze each segment
        results = {}
        tester = HypothesisTester()
        
        for segment, variant_data in segment_variant_metrics.items():
            if len(variant_data) >= 2:  # Need at least 2 variants
                variants = list(variant_data.keys())
                control_values = variant_data[variants[0]]
                treatment_values = variant_data[variants[1]]
                
                if control_values and treatment_values:
                    test_result = tester.two_sample_t_test(control_values, treatment_values)
                    
                    results[segment] = {
                        'control_mean': test_result['control_mean'],
                        'treatment_mean': test_result['treatment_mean'],
                        'difference': test_result['difference'],
                        'relative_change': test_result['relative_change'],
                        'p_value': test_result['p_value'],
                        'significant': test_result['significant'],
                        'sample_sizes': {
                            'control': len(control_values),
                            'treatment': len(treatment_values)
                        }
                    }
        
        return results
    
    def print_segment_report(self, segment_attr: str, metric_name: str):
        """Print formatted segment analysis report."""
        results = self.segment_by_attribute(segment_attr, metric_name)
        
        print(f"Segment Analysis: {metric_name} by {segment_attr}")
        print("=" * 80)
        print(f"\n{'Segment':<20} {'Control':<12} {'Treatment':<12} {'Diff':<10} {'Rel %':<10} {'Sig?'}")
        print("-" * 80)
        
        for segment, stats in sorted(results.items()):
            sig_marker = "âœ“" if stats['significant'] else "âœ—"
            print(f"{segment:<20} {stats['control_mean']:<12.4f} {stats['treatment_mean']:<12.4f} "
                  f"{stats['difference']:<+10.4f} {stats['relative_change']:<+9.2f}% {sig_marker}")
            print(f"{'':20} n={stats['sample_sizes']['control']:,} "
                  f"{'':12} n={stats['sample_sizes']['treatment']:,}")
        
        # Highlight concerning segments
        print("\nâš  Segments with Degradation:")
        degraded = [seg for seg, stats in results.items() 
                   if stats['significant'] and stats['difference'] < 0]
        
        if degraded:
            for seg in degraded:
                stats = results[seg]
                print(f"  â€¢ {seg}: {stats['relative_change']:.2f}% (p={stats['p_value']:.4f})")
        else:
            print("  None - all segments improved or neutral")

# Usage example (with simulated data structure)
# Create sample events with segment metadata
sample_events = []
for i in range(1000):
    # Simulate query_category segment
    categories = ['exact_match', 'conceptual', 'error_message']
    category = random.choice(categories)
    variant = 'control' if i < 500 else 'treatment'
    
    # Simulate metric values (treatment better for conceptual, worse for exact_match)
    if category == 'exact_match':
        ctr = 0.68 if variant == 'control' else 0.64
    elif category == 'conceptual':
        ctr = 0.28 if variant == 'control' else 0.38
    else:
        ctr = 0.52 if variant == 'control' else 0.58
    
    # Add noise
    ctr += random.gauss(0, 0.05)
    ctr = max(0, min(1, ctr))  # Bound between 0 and 1
    
    event = type('Event', (), {
        'variant': variant,
        'metadata': {'query_category': category},
        'metrics': {'ctr': ctr}
    })()
    sample_events.append(event)

# analyzer = SegmentAnalyzer(sample_events)
# analyzer.print_segment_report('query_category', 'ctr')
```

**Segment Analysis Best Practices**:
- **Pre-define segments**: Query type, user type, device, geography
- **Minimum sample size**: â‰¥100 per segment-variant combination
- **Multiple testing correction**: Use Bonferroni if testing many segments (Î± / n_segments)
- **Action**: Don't ship if critical segment degrades significantly

---

### 4. Monitor Experiments Continuously

**Real-Time Monitoring Dashboard**

```python
class ExperimentMonitor:
    """Monitor running A/B test for issues."""
    
    def __init__(self, experiment_id: str, tracker: ExperimentTracker):
        self.experiment_id = experiment_id
        self.tracker = tracker
        self.alerts = []
    
    def check_allocation_balance(self, expected_allocation: dict, 
                                 tolerance: float = 0.02) -> dict:
        """
        Check if traffic allocation matches expectations.
        
        Args:
            expected_allocation: Dict of variant -> expected percentage (0-1)
            tolerance: Acceptable deviation (e.g., 0.02 = 2%)
        
        Returns:
            Alert if allocation imbalanced
        """
        events = self.tracker.get_events(self.experiment_id)
        
        # Count events per variant
        variant_counts = {}
        for event in events:
            variant_counts[event.variant] = variant_counts.get(event.variant, 0) + 1
        
        total = len(events)
        actual_allocation = {v: count / total for v, count in variant_counts.items()}
        
        # Check balance
        imbalanced = []
        for variant, expected in expected_allocation.items():
            actual = actual_allocation.get(variant, 0)
            deviation = abs(actual - expected)
            
            if deviation > tolerance:
                imbalanced.append({
                    'variant': variant,
                    'expected': expected,
                    'actual': actual,
                    'deviation': deviation
                })
        
        if imbalanced:
            alert = {
                'type': 'allocation_imbalance',
                'severity': 'high',
                'message': f"{len(imbalanced)} variant(s) have imbalanced allocation",
                'details': imbalanced
            }
            self.alerts.append(alert)
            return alert
        
        return {'status': 'ok'}
    
    def check_sample_size_progress(self, target_per_variant: int) -> dict:
        """Check if sample size is on track to reach target."""
        events = self.tracker.get_events(self.experiment_id)
        config = self.tracker.get_experiment_config(self.experiment_id)
        
        # Count per variant
        variant_counts = {}
        for event in events:
            variant_counts[event.variant] = variant_counts.get(event.variant, 0) + 1
        
        # Calculate progress
        from datetime import datetime
        start_date = datetime.fromisoformat(config.start_date)
        now = datetime.now()
        days_elapsed = (now - start_date).days + 1
        
        progress_report = {}
        for variant, count in variant_counts.items():
            progress_pct = (count / target_per_variant * 100) if target_per_variant > 0 else 0
            daily_rate = count / days_elapsed
            days_to_target = (target_per_variant - count) / daily_rate if daily_rate > 0 else float('inf')
            
            progress_report[variant] = {
                'current': count,
                'target': target_per_variant,
                'progress_pct': progress_pct,
                'daily_rate': daily_rate,
                'days_to_target': days_to_target
            }
        
        return progress_report
    
    def check_guardrail_metrics(self, guardrails: dict) -> dict:
        """
        Check if guardrail metrics are violated.
        
        Args:
            guardrails: Dict of metric_name -> {'max': value} or {'min': value}
        
        Returns:
            Alerts for violated guardrails
        """
        events = self.tracker.get_events(self.experiment_id)
        
        # Calculate metrics per variant
        variant_metrics = {}
        for event in events:
            if event.variant not in variant_metrics:
                variant_metrics[event.variant] = {}
            
            for metric_name, value in event.metrics.items():
                if metric_name not in variant_metrics[event.variant]:
                    variant_metrics[event.variant][metric_name] = []
                variant_metrics[event.variant][metric_name].append(value)
        
        # Check guardrails
        violations = []
        for variant, metrics in variant_metrics.items():
            for metric_name, constraint in guardrails.items():
                if metric_name in metrics:
                    values = metrics[metric_name]
                    mean_value = np.mean(values)
                    
                    if 'max' in constraint and mean_value > constraint['max']:
                        violations.append({
                            'variant': variant,
                            'metric': metric_name,
                            'value': mean_value,
                            'constraint': f"<= {constraint['max']}",
                            'violation': mean_value - constraint['max']
                        })
                    
                    if 'min' in constraint and mean_value < constraint['min']:
                        violations.append({
                            'variant': variant,
                            'metric': metric_name,
                            'value': mean_value,
                            'constraint': f">= {constraint['min']}",
                            'violation': constraint['min'] - mean_value
                        })
        
        if violations:
            alert = {
                'type': 'guardrail_violation',
                'severity': 'critical',
                'message': f"{len(violations)} guardrail metric(s) violated",
                'details': violations,
                'action': 'Consider stopping experiment'
            }
            self.alerts.append(alert)
            return alert
        
        return {'status': 'ok'}
    
    def generate_monitoring_report(self, expected_allocation: dict,
                                   target_sample_size: int,
                                   guardrails: dict):
        """Generate complete monitoring report."""
        print("Experiment Monitoring Report")
        print("=" * 60)
        print(f"Experiment ID: {self.experiment_id}")
        print(f"Timestamp: {datetime.now().isoformat()}")
        
        # Allocation check
        print("\n1. Allocation Balance:")
        alloc_result = self.check_allocation_balance(expected_allocation)
        if alloc_result.get('status') == 'ok':
            print("  âœ“ Allocation balanced")
        else:
            print(f"  âœ— {alloc_result['message']}")
            for detail in alloc_result['details']:
                print(f"    {detail['variant']}: {detail['actual']:.2%} "
                      f"(expected {detail['expected']:.2%}, deviation {detail['deviation']:.2%})")
        
        # Sample size progress
        print("\n2. Sample Size Progress:")
        progress = self.check_sample_size_progress(target_sample_size)
        for variant, stats in progress.items():
            status = "âœ“" if stats['progress_pct'] >= 100 else "â³"
            print(f"  {status} {variant}: {stats['current']:,} / {stats['target']:,} "
                  f"({stats['progress_pct']:.1f}%)")
            if stats['progress_pct'] < 100:
                print(f"      ETA: {stats['days_to_target']:.1f} days at current rate")
        
        # Guardrails
        print("\n3. Guardrail Metrics:")
        guardrail_result = self.check_guardrail_metrics(guardrails)
        if guardrail_result.get('status') == 'ok':
            print("  âœ“ All guardrails passing")
        else:
            print(f"  âœ— {guardrail_result['message']}")
            for violation in guardrail_result['details']:
                print(f"    {violation['variant']} - {violation['metric']}: "
                      f"{violation['value']:.4f} violates {violation['constraint']}")
        
        # Summary
        print("\n" + "=" * 60)
        if self.alerts:
            print(f"âš  {len(self.alerts)} alert(s) detected")
            for alert in self.alerts:
                print(f"  â€¢ [{alert['severity'].upper()}] {alert['message']}")
        else:
            print("âœ“ All checks passing")

# Usage
# monitor = ExperimentMonitor("exp_semantic_001", tracker)
# 
# monitor.generate_monitoring_report(
#     expected_allocation={'control': 0.5, 'treatment': 0.5},
#     target_sample_size=24847,
#     guardrails={
#         'zero_result_rate': {'max': 0.03},
#         'latency_p95': {'max': 350},
#         'error_rate': {'max': 0.01}
#     }
# )
```

**Monitoring Frequency**:
- **First 24 hours**: Every 4 hours (catch critical issues early)
- **Days 2-7**: Daily
- **Week 2+**: Every 2-3 days

---

### 5. Account for Multiple Testing

**The Multiple Comparisons Problem**

Testing multiple metrics increases false positive rate. With 20 metrics at Î±=0.05, expect 1 false positive even with no real effect.

**Bonferroni Correction**

```python
class MultipleTestingCorrection:
    """Handle multiple testing corrections."""
    
    @staticmethod
    def bonferroni_correction(alpha: float, n_tests: int) -> float:
        """
        Bonferroni correction for multiple tests.
        
        Adjusted Î± = Î± / n_tests
        
        Conservative but simple.
        """
        return alpha / n_tests
    
    @staticmethod
    def benjamini_hochberg(p_values: list, alpha: float = 0.05) -> dict:
        """
        Benjamini-Hochberg procedure (less conservative).
        
        Controls False Discovery Rate (FDR).
        
        Args:
            p_values: List of p-values from multiple tests
            alpha: Desired FDR level
        
        Returns:
            Dict with adjusted significance decisions
        """
        n = len(p_values)
        
        # Sort p-values with original indices
        sorted_p = sorted(enumerate(p_values), key=lambda x: x[1])
        
        # Find largest i where p_i <= (i/n) * alpha
        reject = []
        for i, (original_idx, p) in enumerate(sorted_p, 1):
            threshold = (i / n) * alpha
            if p <= threshold:
                reject.append(original_idx)
        
        # All tests with index <= largest rejected are also rejected
        if reject:
            max_reject_idx = max(reject)
            reject = list(range(max_reject_idx + 1))
        
        return {
            'rejected_indices': reject,
            'adjusted_alpha': (len(reject) / n * alpha) if n > 0 else alpha
        }
    
    @staticmethod
    def print_correction_comparison(metric_names: list, p_values: list, 
                                   alpha: float = 0.05):
        """
        Compare significance with and without correction.
        """
        print(f"Multiple Testing Correction (Î±={alpha})")
        print("=" * 80)
        
        bonferroni_alpha = MultipleTestingCorrection.bonferroni_correction(alpha, len(p_values))
        bh_result = MultipleTestingCorrection.benjamini_hochberg(p_values, alpha)
        
        print(f"\n{'Metric':<25} {'P-value':<12} {'Uncorrected':<15} {'Bonferroni':<15} {'BH FDR'}")
        print("-" * 80)
        
        for i, (metric, p) in enumerate(zip(metric_names, p_values)):
            uncorrected = "Significant" if p < alpha else "Not Sig"
            bonf = "Significant" if p < bonferroni_alpha else "Not Sig"
            bh = "Significant" if i in bh_result['rejected_indices'] else "Not Sig"
            
            print(f"{metric:<25} {p:<12.4f} {uncorrected:<15} {bonf:<15} {bh}")
        
        print(f"\nCorrected Î± (Bonferroni): {bonferroni_alpha:.4f}")
        print(f"Significant tests (Uncorrected): {sum(1 for p in p_values if p < alpha)}")
        print(f"Significant tests (Bonferroni): {sum(1 for p in p_values if p < bonferroni_alpha)}")
        print(f"Significant tests (BH): {len(bh_result['rejected_indices'])}")

# Usage
metrics = ['ctr@10', 'mrr', 'ndcg@10', 'zero_result_rate', 'latency_p95']
p_values = [0.001, 0.042, 0.018, 0.067, 0.350]

MultipleTestingCorrection.print_correction_comparison(metrics, p_values, alpha=0.05)
```

**Best Practice**:
- **Primary metric**: Use uncorrected Î± (this is the hypothesis)
- **Secondary/guardrail metrics**: Apply Bonferroni or BH correction
- **Exploratory metrics**: Report as exploratory, don't make decisions

---

### 6. Plan for Sufficient Test Duration

**Avoid Premature Stopping**

Stopping early when results look good inflates false positive rate ("peeking problem").

**Sequential Testing Framework**

```python
class SequentialTesting:
    """Handle sequential testing with alpha spending."""
    
    @staticmethod
    def obrien_fleming_bounds(alpha: float, n_looks: int) -> list:
        """
        O'Brien-Fleming spending function for sequential testing.
        
        Allows peeking at results without inflating Type I error.
        
        Args:
            alpha: Overall significance level
            n_looks: Number of planned interim analyses
        
        Returns:
            List of adjusted alpha thresholds for each look
        """
        # O'Brien-Fleming uses spending function: alpha * (1 - Î¦(z_alpha/2 / sqrt(t)))
        # Simplified approximation
        thresholds = []
        
        for k in range(1, n_looks + 1):
            t = k / n_looks  # Fraction of information
            adjusted_alpha = alpha * (2 - 2 * stats.norm.cdf(stats.norm.ppf(1 - alpha/2) / np.sqrt(t)))
            thresholds.append(adjusted_alpha)
        
        return thresholds
    
    @staticmethod
    def plan_interim_analyses(total_duration_days: int, n_looks: int = 3) -> list:
        """
        Plan when to conduct interim analyses.
        
        Args:
            total_duration_days: Planned experiment duration
            n_looks: Number of interim analyses (default 3)
        
        Returns:
            List of days for interim analysis
        """
        intervals = [int(total_duration_days * (k / n_looks)) for k in range(1, n_looks + 1)]
        return intervals

# Usage
alpha = 0.05
n_looks = 3  # Check at 33%, 66%, 100% of planned duration

thresholds = SequentialTesting.obrien_fleming_bounds(alpha, n_looks)
interim_days = SequentialTesting.plan_interim_analyses(total_duration_days=14, n_looks=3)

print("Sequential Testing Plan:")
print("=" * 50)
for i, (day, threshold) in enumerate(zip(interim_days, thresholds), 1):
    print(f"Look {i} (Day {day}): Reject if p < {threshold:.4f}")

# Output:
# Sequential Testing Plan:
# ==================================================
# Look 1 (Day 4): Reject if p < 0.0001
# Look 2 (Day 9): Reject if p < 0.0048  
# Look 3 (Day 14): Reject if p < 0.0451
```

**Recommendation**:
- **Minimum duration**: 7 days (1 full week)
- **Optimal**: 14 days (2 weeks, captures weekend patterns)
- **If peeking**: Use sequential testing framework
- **Best**: Pre-commit to duration, analyze only at end

---

### 7. Document Everything

**Experiment Documentation Template**

```python
class ExperimentDocumentation:
    """Template for comprehensive experiment documentation."""
    
    @staticmethod
    def generate_experiment_doc(experiment_id: str, config: ExperimentConfig,
                               results: dict) -> str:
        """Generate complete experiment documentation."""
        
        doc = f"""
# Experiment Documentation: {config.name}

## Experiment ID
{experiment_id}

## Hypothesis
{config.hypothesis}

## Business Context
**Problem**: [Describe the problem being solved]
**Opportunity**: [Describe the expected improvement]
**Stakeholders**: [List key stakeholders]
**Investment Required**: [Engineering time, infrastructure costs]

## Experiment Design

### Metrics
**Primary Metric**: {config.metrics[0] if config.metrics else 'None specified'}
**Secondary Metrics**: {', '.join(config.metrics[1:]) if len(config.metrics) > 1 else 'None'}
**Guardrail Metrics**: [List metrics that must not degrade]

### Variants
{chr(10).join(f"- **{v}**: [Description]" for v in config.variants)}

### Sample Size Calculation
- Baseline: [value]
- Minimum Detectable Effect: [value]
- Significance level (Î±): 0.05
- Power (1-Î²): 0.80
- Required sample size: [n per variant]
- Expected duration: [days]

### Randomization
- Unit: [User ID / Session / Query]
- Method: [Hash-based / Random assignment]
- Allocation: {config.allocations}

## Implementation

### Launch Date
{config.start_date}

### Variant Configurations
[Detailed configuration for each variant]

### Instrumentation
[List tracked events and properties]

### A/A Test Validation
- Date: [date]
- Result: [PASS/FAIL]
- Allocation balance: [percentage]
- Statistical test: p=[value]

## Results

### Sample Sizes
[Actual sample sizes per variant]

### Primary Metric Results
- Control: [value]
- Treatment: [value]
- Difference: [value]
- P-value: [value]
- 95% CI: [lower, upper]
- **Conclusion**: [Significant/Not significant]

### Secondary Metrics
[Results for each secondary metric]

### Segment Analysis
[Results by key segments]

### Guardrail Metrics
[Check all guardrails passed]

## Decision

**Recommendation**: [Ship / Don't Ship / Iterate]

**Rationale**: [Explain decision based on data]

**Risks**: [Any risks if shipping]

**Rollout Plan**: [If shipping, how to roll out]

## Learnings

### What Worked
- [Learning 1]
- [Learning 2]

### What Didn't Work
- [Challenge 1]
- [Challenge 2]

### Surprises
- [Unexpected finding 1]
- [Unexpected finding 2]

## Next Steps
- [Action item 1]
- [Action item 2]

---

**Approved by**: [Name, Date]
**Shipped**: [Yes/No, Date if yes]
**Follow-up experiments**: [List any planned follow-ups]
"""
        
        return doc

# Usage: Generate template for experiment
# doc_template = ExperimentDocumentation.generate_experiment_doc(
#     experiment_id="exp_semantic_001",
#     config=experiment_config,
#     results={}
# )
# print(doc_template)
```

---

### 8. Validate Results Before Shipping

**Pre-Ship Validation Checklist**

```python
class PreShipValidator:
    """Validate experiment before shipping to production."""
    
    def __init__(self, experiment_id: str, analyzer: ExperimentAnalyzer):
        self.experiment_id = experiment_id
        self.analyzer = analyzer
        self.validation_results = []
    
    def run_all_validations(self) -> bool:
        """Run all pre-ship validations."""
        checks = [
            self.check_sample_size_adequate(),
            self.check_statistical_significance(),
            self.check_practical_significance(),
            self.check_guardrails_passing(),
            self.check_segment_consistency(),
            self.check_novelty_effects(),
            self.check_data_quality()
        ]
        
        return all(checks)
    
    def check_sample_size_adequate(self) -> bool:
        """Verify sample size meets requirements."""
        sample_sizes = self.analyzer._calculate_sample_sizes()
        min_required = self.analyzer.config.metadata.get('min_sample_size', 5000)
        
        adequate = all(size >= min_required for size in sample_sizes.values())
        
        self.validation_results.append({
            'check': 'Sample Size',
            'passed': adequate,
            'details': f"Variants: {sample_sizes}, Required: {min_required}"
        })
        
        return adequate
    
    def check_statistical_significance(self) -> bool:
        """Verify primary metric is statistically significant."""
        tests = self.analyzer._run_statistical_tests()
        primary_metric = self.analyzer.config.metrics[0]
        
        significant = tests.get(primary_metric, {}).get('significant', False)
        
        self.validation_results.append({
            'check': 'Statistical Significance',
            'passed': significant,
            'details': f"{primary_metric}: p={tests.get(primary_metric, {}).get('p_value', 'N/A')}"
        })
        
        return significant
    
    def check_practical_significance(self) -> bool:
        """Verify improvement is practically meaningful."""
        tests = self.analyzer._run_statistical_tests()
        primary_metric = self.analyzer.config.metrics[0]
        
        min_practical_improvement = 0.02  # 2% minimum
        
        result = tests.get(primary_metric, {})
        improvement = result.get('relative_change', 0) / 100
        
        meaningful = abs(improvement) >= min_practical_improvement
        
        self.validation_results.append({
            'check': 'Practical Significance',
            'passed': meaningful,
            'details': f"{primary_metric}: {improvement:.2%} (min {min_practical_improvement:.0%})"
        })
        
        return meaningful
    
    def check_guardrails_passing(self) -> bool:
        """Verify no guardrail metrics degraded."""
        tests = self.analyzer._run_statistical_tests()
        
        # Check secondary metrics didn't significantly degrade
        guardrail_passed = True
        for metric in self.analyzer.config.metrics[1:]:  # Skip primary
            if metric in tests:
                result = tests[metric]
                if result['significant'] and result['difference'] < -0.01:
                    guardrail_passed = False
                    break
        
        self.validation_results.append({
            'check': 'Guardrail Metrics',
            'passed': guardrail_passed,
            'details': 'All guardrails within acceptable range' if guardrail_passed else 'Some guardrails failed'
        })
        
        return guardrail_passed
    
    def check_segment_consistency(self) -> bool:
        """Verify key segments don't degrade."""
        # Simplified - would check actual segment analysis
        consistent = True
        
        self.validation_results.append({
            'check': 'Segment Consistency',
            'passed': consistent,
            'details': 'No critical segment degradation detected'
        })
        
        return consistent
    
    def check_novelty_effects(self) -> bool:
        """Check for novelty bias (first week vs second week)."""
        # Simplified - would compare early vs late period metrics
        no_novelty = True
        
        self.validation_results.append({
            'check': 'Novelty Effects',
            'passed': no_novelty,
            'details': 'Effect consistent across time periods'
        })
        
        return no_novelty
    
    def check_data_quality(self) -> bool:
        """Verify data quality is acceptable."""
        events = self.analyzer.events
        
        # Check for missing data
        total_events = len(events)
        events_with_primary_metric = sum(
            1 for e in events 
            if self.analyzer.config.metrics[0] in e.metrics
        )
        
        data_quality_pct = events_with_primary_metric / total_events if total_events > 0 else 0
        acceptable = data_quality_pct >= 0.95  # 95% threshold
        
        self.validation_results.append({
            'check': 'Data Quality',
            'passed': acceptable,
            'details': f"{data_quality_pct:.1%} of events have primary metric"
        })
        
        return acceptable
    
    def print_validation_report(self):
        """Print validation report."""
        print("Pre-Ship Validation Report")
        print("=" * 60)
        print(f"Experiment: {self.experiment_id}")
        
        all_passed = all(r['passed'] for r in self.validation_results)
        
        print(f"\nOverall Status: {'âœ“ READY TO SHIP' if all_passed else 'âœ— NOT READY'}")
        print("\nValidation Checks:")
        
        for result in self.validation_results:
            status = "âœ“ PASS" if result['passed'] else "âœ— FAIL"
            print(f"  {status} {result['check']}")
            print(f"      {result['details']}")
        
        if not all_passed:
            print("\nâš  RECOMMENDATION: Do not ship until all validations pass")
        else:
            print("\nâœ“ All validations passed - safe to proceed with rollout")
        
        return all_passed

# Usage
# validator = PreShipValidator("exp_semantic_001", analyzer)
# ready_to_ship = validator.run_all_validations()
# validator.print_validation_report()
```

---

## Troubleshooting

### 1. Sample Ratio Mismatch (SRM)

**Symptoms**: Traffic allocation significantly deviates from expected (e.g., 52/48 when expecting 50/50).

**Root Causes**:
- Biased randomization algorithm
- Bot traffic filtered differently per variant
- Logging instrumentation issues
- Browser/cache inconsistencies

**Diagnosis**:

```python
def diagnose_srm(expected_allocation: dict, actual_counts: dict, 
                 alpha: float = 0.001) -> dict:
    """
    Diagnose Sample Ratio Mismatch using chi-square test.
    
    Args:
        expected_allocation: Dict of variant -> expected proportion
        actual_counts: Dict of variant -> actual count
        alpha: Significance level (use 0.001 for SRM, stricter than normal)
    
    Returns:
        SRM diagnosis
    """
    total = sum(actual_counts.values())
    
    # Expected counts
    expected_counts = {v: total * prop for v, prop in expected_allocation.items()}
    
    # Chi-square test
    chi_square = sum(
        (actual_counts[v] - expected_counts[v]) ** 2 / expected_counts[v]
        for v in expected_allocation.keys()
    )
    
    df = len(expected_allocation) - 1
    p_value = 1 - stats.chi2.cdf(chi_square, df)
    
    srm_detected = p_value < alpha
    
    return {
        'srm_detected': srm_detected,
        'chi_square': chi_square,
        'p_value': p_value,
        'expected': expected_counts,
        'actual': actual_counts,
        'deviations': {
            v: (actual_counts[v] - expected_counts[v]) / expected_counts[v] * 100
            for v in expected_allocation.keys()
        }
    }

# Usage
expected = {'control': 0.5, 'treatment': 0.5}
actual = {'control': 52340, 'treatment': 47660}  # 52.3% / 47.7%

srm_result = diagnose_srm(expected, actual, alpha=0.001)

print("SRM Diagnosis:")
print(f"  SRM Detected: {'YES âš ' if srm_result['srm_detected'] else 'NO âœ“'}")
print(f"  P-value: {srm_result['p_value']:.6f}")
print("\n  Deviations from Expected:")
for variant, deviation in srm_result['deviations'].items():
    print(f"    {variant}: {deviation:+.2f}%")
```

**Solutions**:

1. **Fix Randomization**:
   - Use hash-based assignment (deterministic)
   - Verify hash distribution is uniform
   - Test with A/A experiment

2. **Filter Bot Traffic Consistently**:
   - Apply same bot detection to all variants
   - Filter before randomization, not after

3. **Check Logging**:
   - Verify all variants log events correctly
   - Check for dropped events in variant code paths

4. **Action**: **Do NOT analyze experiment if SRM detected** - results are unreliable

---

### 2. Novelty/Primacy Effects

**Symptoms**: Treatment wins in first few days, then effect diminishes or reverses.

**Root Causes**:
- Users notice change, engage more initially (novelty)
- Users prefer familiar interface (primacy bias)
- Seasonal/temporal effects

**Diagnosis**:

```python
def detect_novelty_effect(events: list, metric_name: str, 
                          early_days: int = 3) -> dict:
    """
    Detect novelty effect by comparing early vs late period.
    
    Args:
        events: List of experiment events
        metric_name: Metric to analyze
        early_days: Number of days considered "early" period
    
    Returns:
        Comparison of early vs late period
    """
    from datetime import datetime, timedelta
    
    # Find experiment start
    start_time = min(datetime.fromisoformat(e.timestamp) for e in events)
    early_cutoff = start_time + timedelta(days=early_days)
    
    # Split events
    early_events = [e for e in events if datetime.fromisoformat(e.timestamp) < early_cutoff]
    late_events = [e for e in events if datetime.fromisoformat(e.timestamp) >= early_cutoff]
    
    # Analyze each period
    def analyze_period(period_events):
        control = [e.metrics[metric_name] for e in period_events 
                   if e.variant == 'control' and metric_name in e.metrics]
        treatment = [e.metrics[metric_name] for e in period_events 
                    if e.variant == 'treatment' and metric_name in e.metrics]
        
        if control and treatment:
            tester = HypothesisTester()
            result = tester.two_sample_t_test(control, treatment)
            return {
                'control_mean': result['control_mean'],
                'treatment_mean': result['treatment_mean'],
                'difference': result['difference'],
                'relative_change': result['relative_change'],
                'p_value': result['p_value']
            }
        return None
    
    early_result = analyze_period(early_events)
    late_result = analyze_period(late_events)
    
    # Check if effect diminished
    novelty_detected = False
    if early_result and late_result:
        early_lift = early_result['relative_change']
        late_lift = late_result['relative_change']
        
        # Novelty if early effect much larger than late effect
        if early_lift > 5 and late_lift < early_lift * 0.5:
            novelty_detected = True
    
    return {
        'early_period': early_result,
        'late_period': late_result,
        'novelty_detected': novelty_detected
    }

# Example usage would compare Day 1-3 vs Day 8-14
```

**Solutions**:

1. **Extend Test Duration**: Run 2-4 weeks to let novelty wear off
2. **Analyze Late Period Only**: Use days 8-14 for decision, discard early period
3. **Discount Early Period**: Weight later days more heavily in analysis

---

### 3. Low Statistical Power (Inconclusive Results)

**Symptoms**: P-value > 0.05, can't reject null hypothesis, uncertain if treatment actually better.

**Root Causes**:
- Sample size too small
- Effect size smaller than expected
- High variance in metric

**Diagnosis**:

```python
def diagnose_low_power(control_values: list, treatment_values: list,
                       alpha: float = 0.05, target_power: float = 0.80) -> dict:
    """
    Diagnose why test has low power.
    """
    # Observed effect size
    control_mean = np.mean(control_values)
    treatment_mean = np.mean(treatment_values)
    pooled_std = np.sqrt((np.var(control_values) + np.var(treatment_values)) / 2)
    
    observed_effect_size = abs(treatment_mean - control_mean) / pooled_std
    observed_relative_change = (treatment_mean - control_mean) / control_mean * 100 if control_mean != 0 else 0
    
    # Current sample size
    n_current = len(control_values)
    
    # Required sample size for observed effect
    calculator = SampleSizeCalculator()
    n_required = calculator.calculate_for_means(
        control_mean, treatment_mean, pooled_std, alpha, target_power
    )
    
    # Power of current test
    from scipy.stats import t
    ncp = observed_effect_size * np.sqrt(n_current / 2)  # Non-centrality parameter
    df = 2 * n_current - 2
    critical_t = t.ppf(1 - alpha/2, df)
    current_power = 1 - t.cdf(critical_t, df, ncp) + t.cdf(-critical_t, df, ncp)
    
    return {
        'observed_effect_size': observed_effect_size,
        'observed_relative_change': observed_relative_change,
        'current_sample_size': n_current,
        'required_sample_size': n_required,
        'sample_size_shortfall': n_required - n_current,
        'current_power': current_power,
        'target_power': target_power,
        'recommendation': 'Extend test' if n_required > n_current else 'Effect too small to detect reliably'
    }

# Usage
control = [0.75] * 5000  # Simulated
treatment = [0.76] * 5000  # Small effect

power_diagnosis = diagnose_low_power(control, treatment)
print(f"Power Diagnosis:")
print(f"  Observed effect: {power_diagnosis['observed_relative_change']:.2f}%")
print(f"  Current sample: {power_diagnosis['current_sample_size']:,}")
print(f"  Required sample: {power_diagnosis['required_sample_size']:,}")
print(f"  Current power: {power_diagnosis['current_power']:.2f}")
print(f"  Recommendation: {power_diagnosis['recommendation']}")
```

**Solutions**:

1. **Extend Test Duration**: Collect more samples
2. **Increase Traffic Allocation**: Allocate more users to experiment
3. **Accept Lower MDE**: If effect is smaller than expected, decide if still worthwhile
4. **Increase Effect**: Improve treatment variant to have larger impact

---

### 4. Conflicting Metrics (Trade-offs)

**Symptoms**: Primary metric improves but secondary metric degrades.

**Example**: CTR increases but session abandonment also increases (users click but don't find what they need).

**Diagnosis**:

```python
def analyze_metric_tradeoffs(test_results: dict) -> dict:
    """
    Identify metric trade-offs.
    
    Args:
        test_results: Dict of metric_name -> test result
    
    Returns:
        Trade-off analysis
    """
    improvements = []
    degradations = []
    neutral = []
    
    for metric, result in test_results.items():
        if result['significant']:
            if result['difference'] > 0:
                improvements.append({
                    'metric': metric,
                    'change': result['relative_change']
                })
            else:
                degradations.append({
                    'metric': metric,
                    'change': result['relative_change']
                })
        else:
            neutral.append(metric)
    
    has_tradeoff = len(improvements) > 0 and len(degradations) > 0
    
    return {
        'has_tradeoff': has_tradeoff,
        'improvements': improvements,
        'degradations': degradations,
        'neutral': neutral,
        'decision_required': has_tradeoff
    }

# Example
test_results = {
    'ctr': {'significant': True, 'difference': 0.03, 'relative_change': 8.2},
    'abandonment_rate': {'significant': True, 'difference': 0.02, 'relative_change': 12.5},
    'conversion': {'significant': False, 'difference': -0.001, 'relative_change': -0.8}
}

tradeoff = analyze_metric_tradeoffs(test_results)
print(f"Trade-off Detected: {tradeoff['has_tradeoff']}")
if tradeoff['has_tradeoff']:
    print("\nImprovements:")
    for item in tradeoff['improvements']:
        print(f"  + {item['metric']}: {item['change']:+.1f}%")
    print("\nDegradations:")
    for item in tradeoff['degradations']:
        print(f"  - {item['metric']}: {item['change']:+.1f}%")
```

**Solutions**:

1. **Stakeholder Decision**: Which metric is more important? (Business prioritization)
2. **Composite Metric**: Create weighted score combining metrics
3. **Iterate**: Improve treatment to eliminate trade-off
4. **Segment**: Ship to segments where no trade-off exists

---

### 5. External Validity Concerns

**Symptoms**: Results don't match expectations or differ from offline evaluation.

**Root Causes**:
- Online behavior differs from offline dataset
- Experiment population not representative
- Implementation bugs in variant

**Diagnosis**:

```python
def validate_external_consistency(online_results: dict, offline_results: dict,
                                  tolerance: float = 0.10) -> dict:
    """
    Compare online A/B test results with offline evaluation.
    
    Args:
        online_results: Dict of metric -> online result
        offline_results: Dict of metric -> offline result
        tolerance: Acceptable deviation (e.g., 0.10 = 10%)
    
    Returns:
        Consistency check
    """
    discrepancies = []
    
    for metric in online_results.keys():
        if metric in offline_results:
            online_value = online_results[metric]
            offline_value = offline_results[metric]
            
            deviation = abs(online_value - offline_value) / offline_value if offline_value != 0 else 0
            
            if deviation > tolerance:
                discrepancies.append({
                    'metric': metric,
                    'online': online_value,
                    'offline': offline_value,
                    'deviation': deviation * 100
                })
    
    consistent = len(discrepancies) == 0
    
    return {
        'consistent': consistent,
        'discrepancies': discrepancies
    }

# Usage
online = {'ndcg@10': 0.78, 'mrr': 0.82}
offline = {'ndcg@10': 0.85, 'mrr': 0.81}  # Offline overestimated NDCG

consistency = validate_external_consistency(online, offline, tolerance=0.10)
if not consistency['consistent']:
    print("âš  Discrepancies between online and offline results:")
    for disc in consistency['discrepancies']:
        print(f"  {disc['metric']}: Online {disc['online']:.2f}, "
              f"Offline {disc['offline']:.2f} ({disc['deviation']:+.1f}% deviation)")
```

**Solutions**:

1. **Investigate Dataset Bias**: Offline dataset may not represent production queries
2. **Check Implementation**: Verify treatment variant matches offline experiment exactly
3. **Accept Difference**: Online behavior legitimately differs (clicks vs relevance judgments)
4. **Update Offline Dataset**: Use online results to improve dataset for future offline eval

---

## Best Practices

### âœ… Do's
1. **Run A/A test first** to validate instrumentation
2. **Calculate sample size** before starting
3. **Use consistent hashing** for user assignment
4. **Monitor for** imbalanced allocation (SRM detection)
5. **Wait for significance** before making decisions
6. **Document everything** (hypothesis, config, results)
7. **Segment analysis** by user type, query type
8. **Check for novelty effects** (compare early vs late period)
9. **Run for full week minimum** to capture weekly patterns (14 days optimal)
10. **Validate results** before shipping (guardrails, segments, practical significance)
11. **Apply multiple testing correction** for secondary metrics
12. **Plan interim analyses** if peeking (use alpha spending)

### âŒ Don'ts
1. **Don't** peek early and stop tests based on p-values (inflates false positives)
2. **Don't** run multiple tests on same traffic simultaneously (confounding)
3. **Don't** change allocation mid-experiment (invalidates analysis)
4. **Don't** ignore confidence intervals (look beyond p-value for effect size)
5. **Don't** test too many variants (reduces power, use max 3-4)
6. **Don't** run experiments indefinitely (decide and move on)
7. **Don't** forget to account for multiple testing (Bonferroni for guardrails)
8. **Don't** ignore segment differences (aggregate can hide degradations)
9. **Don't** ship without validation (check guardrails, segments, data quality)
10. **Don't** forget to turn off experiment code after shipping (clean up)
11. **Don't** trust results if SRM detected (fix allocation first)
12. **Don't** ignore external validity (compare online vs offline)

---

## Next Steps

- **[Load Testing](./18-load-testing.md)** - Performance testing
- **[Dataset Preparation](./16-dataset-preparation.md)** - Creating test data
- **[Monitoring & Alerting](./23-monitoring-alerting.md)** - Production monitoring

---

*See also: [Scoring Profiles](./14-scoring-profiles.md) | [Index Management](./15-index-management.md)*
